{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgLrIce2ZAaS",
        "outputId": "2639bd1a-28bc-4618-cfb7-76567739ea6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved top_A9_usage.csv\n",
            "Saved top_A2_usage.csv\n",
            "Saved top_A7_usage.csv\n",
            "Saved top_A4_usage.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'API Call Dataset (1).csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "top_apis = data['API Code'].value_counts().head(4).index.tolist()\n",
        "\n",
        "for api in top_apis:\n",
        "    api_data = data[data['API Code'] == api]\n",
        "    file_name = f\"top_{api}_usage.csv\"\n",
        "    api_data.to_csv(file_name, index=False)\n",
        "    print(f\"Saved {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy tensorflow scikit-learn matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_noKNk5Zih0",
        "outputId": "52eca687-9e7d-4283-8099-8dc0509169dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "data = pd.read_csv(\"top_A2_usage.csv\")\n",
        "data['Time of Call'] = pd.to_datetime(data['Time of Call'], dayfirst=True)\n",
        "data.set_index('Time of Call', inplace=True)\n",
        "\n",
        "time_series = data.resample('H').count()['API Code']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "time_series_scaled = scaler.fit_transform(time_series.values.reshape(-1, 1))\n",
        "\n",
        "def create_supervised(data, lag=24):\n",
        "    X, y = [], []\n",
        "    for i in range(lag, len(data)):\n",
        "        X.append(data[i-lag:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lag = 24\n",
        "X, y = create_supervised(time_series_scaled, lag)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd28NNjMujUl",
        "outputId": "81ccac58-37f2-4549-9fdf-4fb41525c94e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ef53207c39ad>:13: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  time_series = data.resample('H').count()['API Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_gru_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(50, activation='relu', input_shape=input_dim))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "5P80WNtD8C80"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_mlp_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_shape=(input_dim[0],)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "aZ1CPu1OvMhh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "def create_lstm_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=input_dim))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "P5I8of-h8KPC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_fcn_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_dim))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZXkK_8-ivXEG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Add, BatchNormalization, Conv1D, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras import Input, Model\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_resnet_model(input_dim):\n",
        "    input_tensor = Input(shape=input_dim)\n",
        "    y = Conv1D(64, kernel_size=3, padding='same', activation='relu')(input_tensor)\n",
        "    y = BatchNormalization()(y)\n",
        "    residual_connection = y\n",
        "\n",
        "    for _ in range(3):\n",
        "        y = Conv1D(64, kernel_size=3, padding='same', activation='relu')(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        y = Add()([y, residual_connection])\n",
        "        residual_connection = y\n",
        "\n",
        "    y = GlobalAveragePooling1D()(y)\n",
        "    output_tensor = Dense(1)(y)\n",
        "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "KTvM4fvmvZEr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Function to train and evaluate models\n",
        "def train_and_evaluate_model(model_creator, X_train_data, y_train_data, X_test_data, y_test_data, model_label):\n",
        "    model_instance = model_creator(X_train_data.shape[1:])  # Create the model instance\n",
        "    training_history = model_instance.fit(X_train_data, y_train_data, epochs=20, batch_size=32, verbose=0, validation_data=(X_test_data, y_test_data))  # Train the model\n",
        "    predictions = model_instance.predict(X_test_data)  # Get predictions from the model\n",
        "    rmse_value = sqrt(mean_squared_error(y_test_data, predictions))  # Calculate RMSE\n",
        "    print(f\"{model_label} RMSE: {rmse_value}\")  # Print RMSE value for each model\n",
        "    return model_instance, rmse_value  # Return trained model and RMSE\n",
        "\n",
        "# Model creation functions (ensure these are defined elsewhere in your code)\n",
        "model_builders = {\n",
        "    \"LSTM\": create_lstm_model,\n",
        "    \"GRU\": create_gru_model,\n",
        "    \"MLP\": create_mlp_model,\n",
        "    \"FCN\": create_fcn_model,\n",
        "    \"ResNet\": create_resnet_model,\n",
        "}\n",
        "\n",
        "# Dictionary to store evaluation results (RMSE for each model)\n",
        "evaluation_results = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, builder_function in model_builders.items():\n",
        "    trained_model, rmse_score = train_and_evaluate_model(builder_function, X_train, y_train, X_test, y_test, model_name)\n",
        "    evaluation_results[model_name] = rmse_score  # Store RMSE for each model\n",
        "\n",
        "# Print the performance of all models\n",
        "print(\"Model Performance:\")\n",
        "for model_label, rmse_score in sorted(evaluation_results.items(), key=lambda x: x[1]):  # Sort models by RMSE value\n",
        "    print(f\"{model_label}: RMSE = {rmse_score}\")  # Print RMSE for each model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y75A_bz9vbLN",
        "outputId": "a3fa0c3d-e7c4-44ab-e156-abb31cd8a42c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "LSTM RMSE: 0.19058193641871415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
            "GRU RMSE: 0.19065641431494715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "MLP RMSE: 0.208667167482576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "FCN RMSE: 0.19064173135229717\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "ResNet RMSE: 0.19747718212228776\n",
            "Model Performance:\n",
            "LSTM: RMSE = 0.19058193641871415\n",
            "FCN: RMSE = 0.19064173135229717\n",
            "GRU: RMSE = 0.19065641431494715\n",
            "ResNet: RMSE = 0.19747718212228776\n",
            "MLP: RMSE = 0.208667167482576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model based on RMSE\n",
        "best_model_name = min(evaluation_results, key=evaluation_results.get)  # Model with the lowest RMSE is considered the best\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model using the entire training dataset\n",
        "best_model = model_builders[best_model_name](X_train.shape[1:])  # Create the best model instance\n",
        "best_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)  # Train the model on the entire dataset\n",
        "\n",
        "# Start with the last observed input from the test set for prediction\n",
        "current_input = X_test[-1].reshape(1, -1, 1)  # Reshaping the input for the model\n",
        "\n",
        "# Define time horizons for the prediction (next hour, next day, next week, next month)\n",
        "horizons = [1, 24, 24 * 7, 24 * 30]  # 1 hour, 24 hours (1 day), 168 hours (1 week), 720 hours (1 month)\n",
        "\n",
        "# Initialize an empty list to store the future predictions\n",
        "future_predictions = []\n",
        "\n",
        "# Sequential prediction for specific horizons\n",
        "for horizon in horizons:\n",
        "    for _ in range(horizon - len(future_predictions)):  # Generate predictions for each horizon\n",
        "        prediction = best_model.predict(current_input, verbose=0)  # Make the next step prediction\n",
        "        future_predictions.append(prediction[0, 0])  # Append predicted value to the list\n",
        "        # Update the input for the next prediction by shifting the sequence\n",
        "        current_input = np.append(current_input[0, 1:], prediction).reshape(1, -1, 1)\n",
        "\n",
        "# Inverse scaling to get the actual prediction values\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Calculate cumulative totals for different time horizons\n",
        "next_hour = round(future_predictions[0][0])  # Single prediction for the next hour\n",
        "next_day = round(sum(future_predictions[:24, 0]))  # Total for the first 24 predictions (24 hours)\n",
        "next_week = round(sum(future_predictions[:24 * 7, 0]))  # Total for the first 7 days (168 hours)\n",
        "next_month = round(sum(future_predictions[:24 * 30, 0]))  # Total for the first 30 days (720 hours)\n",
        "\n",
        "# Print the future predictions for different time horizons\n",
        "print(f\"Next Hour Prediction (Calls): {next_hour}\")\n",
        "print(f\"Total Calls Next Day (Day 1, 24 hours): {next_day}\")\n",
        "print(f\"Total Calls Next Week (Days 1-7): {next_week}\")\n",
        "print(f\"Total Calls Next Month (Days 1-30): {next_month}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoFWAHB9wdWj",
        "outputId": "401bb8d4-4618-4bbc-8612-e9f8c2b33119"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next Hour Prediction (Calls): 1\n",
            "Total Calls Next Day (Day 1, 24 hours): 14\n",
            "Total Calls Next Week (Days 1-7): 103\n",
            "Total Calls Next Month (Days 1-30): 443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset (replace with your file path)\n",
        "data = pd.read_csv(\"top_A4_usage.csv\")  # Replace with actual file\n",
        "data['Time of Call'] = pd.to_datetime(data['Time of Call'],dayfirst=True)\n",
        "data.set_index('Time of Call', inplace=True)\n",
        "\n",
        "# Aggregate API calls by time (e.g., hourly)\n",
        "time_series = data.resample('H').count()['API Code']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "time_series_scaled = scaler.fit_transform(time_series.values.reshape(-1, 1))\n",
        "\n",
        "# Create supervised learning dataset\n",
        "def create_supervised(data, lag=24):\n",
        "    X, y = [], []\n",
        "    for i in range(lag, len(data)):\n",
        "        X.append(data[i-lag:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lag = 24  # Use the last 24 hours to predict the next step\n",
        "X, y = create_supervised(time_series_scaled, lag)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-5q_K3uzWGu",
        "outputId": "52923ac4-3b34-47ba-ce73-b7b40ac8db75"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-34265028b870>:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  time_series = data.resample('H').count()['API Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, Conv1D, GlobalAveragePooling1D, Dropout, concatenate\n",
        "\n",
        "def build_lstm_fcn(input_shape):\n",
        "    # LSTM branch\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    lstm_out = LSTM(64)(input_layer)\n",
        "\n",
        "    # FCN branch\n",
        "    conv1 = Conv1D(128, kernel_size=8, activation='relu', padding='same')(input_layer)\n",
        "    conv2 = Conv1D(256, kernel_size=5, activation='relu', padding='same')(conv1)\n",
        "    conv3 = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv2)\n",
        "    gap = GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    # Merge branches\n",
        "    merged = concatenate([lstm_out, gap])\n",
        "    output_layer = Dense(1)(merged)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "VjdYd92exjYS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gru_fcn(input_shape):\n",
        "    # GRU branch\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    gru_out = GRU(64)(input_layer)\n",
        "\n",
        "    # FCN branch\n",
        "    conv1 = Conv1D(128, kernel_size=8, activation='relu', padding='same')(input_layer)\n",
        "    conv2 = Conv1D(256, kernel_size=5, activation='relu', padding='same')(conv1)\n",
        "    conv3 = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv2)\n",
        "    gap = GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    # Merge branches\n",
        "    merged = concatenate([gru_out, gap])\n",
        "    output_layer = Dense(1)(merged)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "00BZGcIK7fKy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyWavelets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp-SwJpR8FjN",
        "outputId": "eead9c1b-62a9-490e-9111-0364a15a7148"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from PyWavelets) (1.26.4)\n",
            "Downloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pywt\n",
        "\n",
        "def wavelet_decomposition(data, wavelet='db1', level=3):\n",
        "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
        "    reconstructed = pywt.waverec(coeffs, wavelet)\n",
        "    return np.array(coeffs).T  # Transpose for model input\n",
        "\n",
        "# Example model\n",
        "def build_mwdn(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
        "    x = LSTM(64, return_sequences=True)(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    output_layer = Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "aj1CrJKE7fWz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LayerNormalization, Activation\n",
        "\n",
        "def build_tcn(input_shape, nb_filters=64, kernel_size=3, nb_stacks=1, dilation_rates=[1, 2, 4, 8]):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = input_layer\n",
        "\n",
        "    for _ in range(nb_stacks):\n",
        "        for dilation_rate in dilation_rates:\n",
        "            x = Conv1D(nb_filters, kernel_size, padding=\"causal\", dilation_rate=dilation_rate)(x)\n",
        "            x = LayerNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    output_layer = Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "NE-hskYO7zVW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlstm_fcn(input_shape):\n",
        "    # LSTM branch\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    lstm_out = LSTM(64, return_sequences=True)(input_layer)\n",
        "    lstm_out = LSTM(32)(lstm_out)\n",
        "\n",
        "    # FCN branch\n",
        "    conv1 = Conv1D(128, kernel_size=8, activation='relu', padding='same')(input_layer)\n",
        "    conv2 = Conv1D(256, kernel_size=5, activation='relu', padding='same')(conv1)\n",
        "    conv3 = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv2)\n",
        "    gap = GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    # Merge branches\n",
        "    merged = concatenate([lstm_out, gap])\n",
        "    output_layer = Dense(1)(merged)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "i6uUXGd57fel"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def train_and_evaluate(model_builder, X_train, y_train, X_test, y_test, model_name):\n",
        "    model = model_builder(X_train.shape[1:])\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"{model_name} RMSE: {rmse}\")\n",
        "    return model, rmse\n",
        "\n",
        "# Train and evaluate all models\n",
        "models = {\n",
        "    \"LSTM_FCN\": build_lstm_fcn,\n",
        "    \"GRU-FCN\": build_gru_fcn,\n",
        "    \"mWDN\": build_mwdn,\n",
        "    \"TCN \": build_tcn,\n",
        "    \"MLSTM-FCN\": build_mlstm_fcn,\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, builder in models.items():\n",
        "    model, rmse = train_and_evaluate(builder, X_train, y_train, X_test, y_test, name)\n",
        "    results[name] = rmse\n",
        "\n",
        "# Print results\n",
        "print(\"Model Performance:\")\n",
        "for model_name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
        "    print(f\"{model_name}: RMSE = {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDEhKmBz7fg8",
        "outputId": "f8eebfdb-27bf-4992-a3a2-65b0bcab7b0b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "LSTM_FCN RMSE: 0.15152736916406825\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "GRU-FCN RMSE: 0.1519717502606059\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
            "mWDN RMSE: 0.1507166762902506\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
            "TCN  RMSE: 0.15325445536788024\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "MLSTM-FCN RMSE: 0.15565109540219674\n",
            "Model Performance:\n",
            "mWDN: RMSE = 0.1507166762902506\n",
            "LSTM_FCN: RMSE = 0.15152736916406825\n",
            "GRU-FCN: RMSE = 0.1519717502606059\n",
            "TCN : RMSE = 0.15325445536788024\n",
            "MLSTM-FCN: RMSE = 0.15565109540219674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model\n",
        "best_model_name = min(results, key=results.get)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model on the entire dataset\n",
        "best_model = models[best_model_name](X_train.shape[1:])\n",
        "best_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Start with the last observed input\n",
        "current_input = X_test[-1].reshape(1, -1, 1)\n",
        "\n",
        "# Define time horizons\n",
        "horizons = [1, 24, 24 * 7, 24 * 30]  # Next hour, day, week, year\n",
        "\n",
        "# Sequential prediction for specific horizons\n",
        "future_predictions = []\n",
        "for horizon in horizons:\n",
        "    for _ in range(horizon - len(future_predictions)):\n",
        "        prediction = best_model.predict(current_input, verbose=0)\n",
        "        future_predictions.append(prediction[0, 0])\n",
        "        current_input = np.append(current_input[0, 1:], prediction).reshape(1, -1, 1)\n",
        "\n",
        "\n",
        "\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Calculate cumulative totals\n",
        "next_hour = round(future_predictions[0][0])  # Single prediction for the next hour\n",
        "next_day = round(sum(future_predictions[:24, 0]))  # Total for the first 24 predictions (Day 1)\n",
        "next_week = round(sum(future_predictions[:24 * 7, 0]))  # Total for Days 1-7 (Week)\n",
        "next_month = round(sum(future_predictions[:24 * 30, 0]))  # Total for Days 1-30 (Month)\n",
        "\n",
        "# Print results\n",
        "print(f\"Next Hour Prediction (Calls): {next_hour}\")\n",
        "print(f\"Total Calls Next Day (Day 1, 24 hours): {next_day}\")\n",
        "print(f\"Total Calls Next Week (Days 1-7): {next_week}\")\n",
        "print(f\"Total Calls Next Month (Days 1-30): {next_month}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiV4U17H7fka",
        "outputId": "ebb6102a-688f-4cde-c946-acf459478494"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: mWDN\n",
            "Next Hour Prediction (Calls): 1\n",
            "Total Calls Next Day (Day 1, 24 hours): 16\n",
            "Total Calls Next Week (Days 1-7): 104\n",
            "Total Calls Next Month (Days 1-30): 444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset (replace with your file path)\n",
        "data = pd.read_csv(\"top_A7_usage.csv\")  # Replace with actual file\n",
        "data['Time of Call'] = pd.to_datetime(data['Time of Call'],dayfirst=True)\n",
        "data.set_index('Time of Call', inplace=True)\n",
        "\n",
        "# Aggregate API calls by time (e.g., hourly)\n",
        "time_series = data.resample('H').count()['API Code']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "time_series_scaled = scaler.fit_transform(time_series.values.reshape(-1, 1))\n",
        "\n",
        "# Create supervised learning dataset\n",
        "def create_supervised(data, lag=24):\n",
        "    X, y = [], []\n",
        "    for i in range(lag, len(data)):\n",
        "        X.append(data[i-lag:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lag = 24  # Use the last 24 hours to predict the next step\n",
        "X, y = create_supervised(time_series_scaled, lag)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqFD9Ga0_bED",
        "outputId": "f6233202-556c-48f5-ade2-1dce258b83e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-6e7d9c243c4b>:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  time_series = data.resample('H').count()['API Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_inceptiontime(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # First branch: 1x1 convolution\n",
        "    branch1 = layers.Conv1D(32, 1, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # Second branch: 3x3 convolution\n",
        "    branch2 = layers.Conv1D(32, 3, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # Third branch: 5x5 convolution\n",
        "    branch3 = layers.Conv1D(32, 5, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # Concatenate all branches\n",
        "    concatenated = layers.concatenate([branch1, branch2, branch3])\n",
        "\n",
        "    # Global average pooling and dense layer\n",
        "    x = layers.GlobalAveragePooling1D()(concatenated)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)  # Change to 'Dense(num_classes, activation=\"softmax\")' for classification\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "85ud20Yn_e5Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_xceptiontime(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Xception block\n",
        "    x = layers.Conv1D(32, 3, padding='same', activation='relu')(input_layer)\n",
        "    x = layers.SeparableConv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.SeparableConv1D(128, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Global average pooling and dense layer\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)  # For regression, change for classification\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SJReAGLF_fZF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rescnn(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # First convolutional block with residual connections\n",
        "    x = layers.Conv1D(32, 3, padding='same', activation='relu')(input_layer)\n",
        "    x = layers.Conv1D(32, 3, padding='same', activation='relu')(x)\n",
        "    residual = layers.Conv1D(32, 1, padding='same')(input_layer)  # Residual connection\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # Second convolutional block\n",
        "    x = layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    residual = layers.Conv1D(64, 1, padding='same')(x)  # Residual connection\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # Global average pooling and dense layer\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)  # For regression, change for classification\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "oMcBhQtK_oKO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_xcm(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
        "    x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "veyCFoVu_oMN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tst(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "idipe39ZYQ-f"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def train_and_evaluate(model_builder, X_train, y_train, X_test, y_test, model_name):\n",
        "    model = model_builder(X_train.shape[1:])\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"{model_name} RMSE: {rmse}\")\n",
        "    return model, rmse\n",
        "\n",
        "# Train and evaluate all models\n",
        "models = {\n",
        "    \"InceptionTime\": build_inceptiontime,\n",
        "    \"XceptionTime\": build_xceptiontime,\n",
        "    \"ResCNN\": build_rescnn,\n",
        "    \"TST\": build_tst,\n",
        "    \"XCM\": build_xcm,\n",
        "\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, builder in models.items():\n",
        "    model, rmse = train_and_evaluate(builder, X_train, y_train, X_test, y_test, name)\n",
        "    results[name] = rmse\n",
        "\n",
        "# Print results\n",
        "print(\"Model Performance:\")\n",
        "for model_name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
        "    print(f\"{model_name}: RMSE = {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcjPncKL_oPp",
        "outputId": "bcffa6e2-9b95-429a-eed1-52f86f00e724"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "InceptionTime RMSE: 0.19238481512475636\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "XceptionTime RMSE: 0.19082061590736465\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "ResCNN RMSE: 0.19146830757871952\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
            "TST RMSE: 0.1908141198645721\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "XCM RMSE: 0.19245765850154187\n",
            "Model Performance:\n",
            "TST: RMSE = 0.1908141198645721\n",
            "XceptionTime: RMSE = 0.19082061590736465\n",
            "ResCNN: RMSE = 0.19146830757871952\n",
            "InceptionTime: RMSE = 0.19238481512475636\n",
            "XCM: RMSE = 0.19245765850154187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model\n",
        "best_model_name = min(results, key=results.get)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model on the entire dataset\n",
        "best_model = models[best_model_name](X_train.shape[1:])\n",
        "best_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Start with the last observed input\n",
        "current_input = X_test[-1].reshape(1, -1, 1)\n",
        "\n",
        "# Define time horizonsa\n",
        "horizons = [1, 24, 24 * 7, 24 * 30]  # Next hour, day, week, year\n",
        "\n",
        "# Sequential prediction for specific horizons\n",
        "future_predictions = []\n",
        "for horizon in horizons:\n",
        "    for _ in range(horizon - len(future_predictions)):\n",
        "        prediction = best_model.predict(current_input, verbose=0)\n",
        "        future_predictions.append(prediction[0, 0])\n",
        "        current_input = np.append(current_input[0, 1:], prediction).reshape(1, -1, 1)\n",
        "\n",
        "\n",
        "\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Calculate cumulative totals\n",
        "next_hour = round(future_predictions[0][0])  # Single prediction for the next hour\n",
        "next_day = round(sum(future_predictions[:24, 0]))  # Total for the first 24 predictions (Day 1)\n",
        "next_week = round(sum(future_predictions[:24 * 7, 0]))  # Total for Days 1-7 (Week)\n",
        "next_month = round(sum(future_predictions[:24 * 30, 0]))  # Total for Days 1-30 (Month)\n",
        "\n",
        "# Print results\n",
        "print(f\"Next Hour Prediction (Calls): {next_hour}\")\n",
        "print(f\"Total Calls Next Day (Day 1, 24 hours): {next_day}\")\n",
        "print(f\"Total Calls Next Week (Days 1-7): {next_week}\")\n",
        "print(f\"Total Calls Next Month (Days 1-30): {next_month}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9PwTSOnAxnn",
        "outputId": "6dac161b-5f39-4b11-c5b9-c71fc5af4dcd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: TST\n",
            "Next Hour Prediction (Calls): 1\n",
            "Total Calls Next Day (Day 1, 24 hours): 16\n",
            "Total Calls Next Week (Days 1-7): 111\n",
            "Total Calls Next Month (Days 1-30): 477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset (replace with your file path)\n",
        "data = pd.read_csv(\"top_A9_usage.csv\")  # Replace with actual file\n",
        "data['Time of Call'] = pd.to_datetime(data['Time of Call'],dayfirst=True)\n",
        "data.set_index('Time of Call', inplace=True)\n",
        "\n",
        "# Aggregate API calls by time (e.g., hourly)\n",
        "time_series = data.resample('H').count()['API Code']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "time_series_scaled = scaler.fit_transform(time_series.values.reshape(-1, 1))\n",
        "\n",
        "# Create supervised learning dataset\n",
        "def create_supervised(data, lag=24):\n",
        "    X, y = [], []\n",
        "    for i in range(lag, len(data)):\n",
        "        X.append(data[i-lag:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lag = 24  # Use the last 24 hours to predict the next step\n",
        "X, y = create_supervised(time_series_scaled, lag)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMt44yDCJHIN",
        "outputId": "726795e1-6bc3-4b1f-e59f-bf7b966201b1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-037b1aba7044>:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  time_series = data.resample('H').count()['API Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tabtransformer(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "m7n4WWJcJMa1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tsit(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "X1tqtydOJMcl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_gmlp(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Dense(128, activation='relu')(input_layer)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "gq8olEkoJMjr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tsperceiver(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "IKg_zbs6JMnJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gated_tabtransformer(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "yS8Q9MSbTaVp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tssequencerplus(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.LSTM(128, return_sequences=True)(input_layer)\n",
        "    x = layers.LSTM(64)(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "g6jY1kX5JnsM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_patchtst(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "wG-CdmMsOBXc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def train_and_evaluate(model_builder, X_train, y_train, X_test, y_test, model_name):\n",
        "    model = model_builder(X_train.shape[1:])\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"{model_name} RMSE: {rmse}\")\n",
        "    return model, rmse\n",
        "\n",
        "# Train and evaluate all models\n",
        "#\n",
        "\n",
        "\n",
        "models = {\n",
        "\n",
        "\n",
        "     \"PatchTST\": build_patchtst,\n",
        "    \"TabTransformer\": build_tabtransformer,\n",
        "    \"TSPerceiver\": build_tsperceiver,\n",
        "    \"GatedTabTransformer\": build_gated_tabtransformer,\n",
        "    \"TSSequencerPlus\": build_tssequencerplus\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, builder in models.items():\n",
        "    model, rmse = train_and_evaluate(builder, X_train, y_train, X_test, y_test, name)\n",
        "    results[name] = rmse\n",
        "\n",
        "# Print results\n",
        "print(\"Model Performance:\")\n",
        "for model_name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
        "    print(f\"{model_name}: RMSE = {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6Ttc_5lJ0wA",
        "outputId": "5904368b-ad76-4005-e76e-5b7214c432db"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "PatchTST RMSE: 0.15860510679150563\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
            "TabTransformer RMSE: 0.15808687220778986\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
            "TSPerceiver RMSE: 0.15829927685658812\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
            "GatedTabTransformer RMSE: 0.15822544695369975\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "TSSequencerPlus RMSE: 0.15857893262855827\n",
            "Model Performance:\n",
            "TabTransformer: RMSE = 0.15808687220778986\n",
            "GatedTabTransformer: RMSE = 0.15822544695369975\n",
            "TSPerceiver: RMSE = 0.15829927685658812\n",
            "TSSequencerPlus: RMSE = 0.15857893262855827\n",
            "PatchTST: RMSE = 0.15860510679150563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model\n",
        "best_model_name = min(results, key=results.get)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model on the entire dataset\n",
        "best_model = models[best_model_name](X_train.shape[1:])\n",
        "best_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Start with the last observed input\n",
        "current_input = X_test[-1].reshape(1, -1, 1)\n",
        "\n",
        "# Define time horizons\n",
        "horizons = [1, 24, 24 * 7, 24 * 30]  # Next hour, day, week, year\n",
        "\n",
        "# Sequential prediction for specific horizons\n",
        "future_predictions = []\n",
        "for horizon in horizons:\n",
        "    for _ in range(horizon - len(future_predictions)):\n",
        "        prediction = best_model.predict(current_input, verbose=0)\n",
        "        future_predictions.append(prediction[0, 0])\n",
        "        current_input = np.append(current_input[0, 1:], prediction).reshape(1, -1, 1)\n",
        "\n",
        "\n",
        "\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Calculate cumulative totals\n",
        "next_hour = round(future_predictions[0][0])  # Single prediction for the next hour\n",
        "next_day = round(sum(future_predictions[:24, 0]))  # Total for the first 24 predictions (Day 1)\n",
        "next_week = round(sum(future_predictions[:24 * 7, 0]))  # Total for Days 1-7 (Week)\n",
        "next_month = round(sum(future_predictions[:24 * 30, 0]))  # Total for Days 1-30 (Month)\n",
        "\n",
        "# Print results\n",
        "print(f\"Next Hour Prediction (Calls): {next_hour}\")\n",
        "print(f\"Total Calls Next Day (Day 1, 24 hours): {next_day}\")\n",
        "print(f\"Total Calls Next Week (Days 1-7): {next_week}\")\n",
        "print(f\"Total Calls Next Month (Days 1-30): {next_month}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoSoHQuAOCUa",
        "outputId": "27bf5c1d-5c18-488a-b27d-a27dc8fa225e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: TabTransformer\n",
            "Next Hour Prediction (Calls): 1\n",
            "Total Calls Next Day (Day 1, 24 hours): 17\n",
            "Total Calls Next Week (Days 1-7): 118\n",
            "Total Calls Next Month (Days 1-30): 505\n"
          ]
        }
      ]
    }
  ]
}